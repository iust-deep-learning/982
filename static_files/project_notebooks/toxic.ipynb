{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "562047bb-4b28-4d07-ac9c-c6af7e7b068b",
    "_cell_guid": "ebdc6d6a-9373-4df4-a5cf-d95f702d6322",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "import transformers\n",
    "from tqdm.notebook import tqdm\n",
    "from tokenizers import BertWordPieceTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7b17892f-b1cd-4e1d-b815-84c4451009be",
    "_cell_guid": "ef17d93a-169e-4d5c-9f60-f6b2b61d9f52",
    "trusted": true
   },
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "568204d1-ed08-4b91-9f44-f8c1273e7fea",
    "_cell_guid": "e74cdccb-66b7-4dce-869a-d51d3cc2f703",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def fast_encode(dataset, tokenizer, chunk_size=256, maxlen=512):\n",
    "    \"\"\"\n",
    "    https://www.kaggle.com/xhlulu/jigsaw-tpu-distilbert-with-huggingface-and-keras\n",
    "    \"\"\"\n",
    "    tokenizer.enable_truncation(max_length=maxlen)\n",
    "    tokenizer.enable_padding(max_length=maxlen)\n",
    "    lang_map = {\n",
    "        \"en\":0,\n",
    "        \"es\":1\n",
    "    }\n",
    "    all_ids = []\n",
    "    all_langs = []\n",
    "    for i in tqdm(range(0, len(dataset[\"comment_text\"].values), chunk_size)):\n",
    "        text_chunk = dataset[\"comment_text\"][i:i+chunk_size].tolist()\n",
    "        lang_chunk = dataset[\"lang\"][i:i+chunk_size].tolist()\n",
    "        encs = tokenizer.encode_batch(text_chunk)\n",
    "        all_ids.extend([enc.ids for enc in encs])\n",
    "        all_langs.extend([lang_map[i] for i in lang_chunk])\n",
    "    \n",
    "    return np.array(all_ids), np.array(all_langs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def fast_encode_en(dataset, tokenizer, chunk_size=256, maxlen=512):\n",
    "    \"\"\"\n",
    "    https://www.kaggle.com/xhlulu/jigsaw-tpu-distilbert-with-huggingface-and-keras\n",
    "    \"\"\"\n",
    "    tokenizer.enable_truncation(max_length=maxlen)\n",
    "    tokenizer.enable_padding(max_length=maxlen)\n",
    "    lang_map = {\n",
    "        \"en\":0,\n",
    "        \"es\":0\n",
    "    }\n",
    "    all_ids = []\n",
    "    all_langs = []\n",
    "    for i in tqdm(range(0, len(dataset[\"comment_text\"].values), chunk_size)):\n",
    "        text_chunk = dataset[\"comment_text\"][i:i+chunk_size].tolist()\n",
    "        lang_chunk = dataset[\"lang\"][i:i+chunk_size].tolist()\n",
    "        encs = tokenizer.encode_batch(text_chunk)\n",
    "        all_ids.extend([enc.ids for enc in encs])\n",
    "        all_langs.extend([lang_map[i] for i in lang_chunk])\n",
    "    \n",
    "    return np.array(all_ids), np.array(all_langs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dc8a5cb9-cf07-4309-84a4-f03ca9f85a9e",
    "_cell_guid": "0838dd8c-527b-4e26-b2e6-1f5de803b59e",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def build_model(transformer, max_len=512):\n",
    "    \"\"\"\n",
    "    https://www.kaggle.com/xhlulu/jigsaw-tpu-distilbert-with-huggingface-and-keras\n",
    "    \"\"\"\n",
    "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    input_lang = Input(shape=(1,), dtype=tf.int32, name=\"input_lang\")\n",
    "    \n",
    "    sequence_output = transformer(input_word_ids)[0]\n",
    "    cls_token = sequence_output[:, 0, :]\n",
    "    hidden = Dense(100, activation='linear')(cls_token)\n",
    "    out = Dense(1, activation='sigmoid')(hidden)\n",
    "    model = Model(inputs=[input_word_ids,input_lang] , outputs=out)\n",
    "    \n",
    "    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', loss_weights=[1, 2], metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def build_clf_puller_model(tuned_model, max_len=512):\n",
    "    \"\"\"\n",
    "    https://www.kaggle.com/xhlulu/jigsaw-tpu-distilbert-with-huggingface-and-keras\n",
    "    \"\"\"\n",
    "    cls_token = tuned_model.get_layer(\"tf_bert_model\").output[0][:, 0, :]\n",
    "    model = Model(inputs=tuned_model.inputs , outputs=cls_token)\n",
    "    \n",
    "    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def build_language_senstive_model(tuned_model, scale_rotate, vec_to_center, vec_to_translate, max_len=512):\n",
    "    \n",
    "    for layer in tuned_model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    cls_token = tuned_model.get_layer(\"tf_bert_model\").output[0][:, 0, :]\n",
    "    lang =  tuned_model.inputs[1]\n",
    "    print (lang[:].shape)\n",
    "    SR = tf.constant(scale_rotate)\n",
    "    VTC = tf.constant(vec_to_center)\n",
    "#     print (tf.gather(VTC, lang).shape)\n",
    "    VTT = tf.constant(vec_to_translate)\n",
    "    centered = cls_token - tf.gather(VTC, lang[:,0])\n",
    "#     print (centered.shape)    \n",
    "\n",
    "    transformed = tf.linalg.matmul (tf.expand_dims(centered, 1), tf.gather(SR, lang[:,0]))\n",
    "#     print (tf.expand_dims(centered, 1).shape, tf.gather(SR, lang[:,0]).shape)    \n",
    "#     print (transformed.shape)    \n",
    "#     print (tf.squeeze(transformed, 1).shape, tf.gather(VTT, lang[:,0]).shape)    \n",
    "    \n",
    "    translated = tf.squeeze(transformed, 1) + VTT\n",
    "\n",
    "    hidden = Dense(100, activation='linear')(translated)\n",
    "    out = Dense(1, activation='sigmoid')(hidden)\n",
    "\n",
    "    model = Model(inputs=tuned_model.inputs , outputs=out)\n",
    "    \n",
    "    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9899294d-2455-4388-a626-599396420e06",
    "_cell_guid": "c0a4dbe7-a5ae-4cbd-8151-8a6a8eae61ba",
    "trusted": true
   },
   "source": [
    "## TPU Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d67c143a-eab3-42f3-8d97-ca81935d3533",
    "_cell_guid": "e92fc354-540e-4583-bb41-921092dd2572",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Detect hardware, return appropriate distribution strategy\n",
    "try:\n",
    "    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
    "    # set: this is always the case on Kaggle.\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "500c33c3-0abd-4728-9eff-209f9de12e5d",
    "_cell_guid": "b8343386-8840-4bbd-8bd0-78f15b5c3590",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "# Data access\n",
    "GCS_DS_PATH = KaggleDatasets().get_gcs_path()\n",
    "\n",
    "# Configuration\n",
    "EPOCHS = 1\n",
    "BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n",
    "MAX_LEN = 192"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5c797206-7740-4e45-b3ff-3a65a2cc702b",
    "_cell_guid": "0e5de2e0-be60-4580-a5a9-37c99d9f64c0",
    "trusted": true
   },
   "source": [
    "## Create fast tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e72019b8-2d20-4b8f-b920-a13bc6049eaf",
    "_cell_guid": "a7abee00-0225-4dd6-a77a-6ece65322a14",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# First load the real tokenizer\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "# Save the loaded tokenizer locally\n",
    "tokenizer.save_pretrained('.')\n",
    "# Reload it with the huggingface tokenizers library\n",
    "fast_tokenizer = BertWordPieceTokenizer('vocab.txt', lowercase=False)\n",
    "fast_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dd1413a9-25e7-4a32-86c9-dba867ca3439",
    "_cell_guid": "130bb105-68e1-4626-8005-2e34c8023379",
    "trusted": true
   },
   "source": [
    "## Load text data into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f53638df-9a88-4892-a29a-b06d1d97a19e",
    "_cell_guid": "eec7a2e9-dc89-48a2-8fde-0d676c78f695",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "############# train\n",
    "train1 = pd.read_csv(\"/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv\")\n",
    "train2 = pd.read_csv(\"/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train.csv\")\n",
    "train2.toxic = train2.toxic.round().astype(int)  \n",
    "\n",
    "train = pd.concat([\n",
    "    train1[['comment_text', 'toxic']],\n",
    "    train2[['comment_text', 'toxic']].query('toxic==1'),\n",
    "    train2[['comment_text', 'toxic']].query('toxic==0').sample(n=60000, random_state=0)\n",
    "])\n",
    "\n",
    "\n",
    "lang = [\"en\"]*train.shape[0]\n",
    "train[\"lang\"] = lang\n",
    "\n",
    "\n",
    "############# test\n",
    "orig_valid = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation.csv')\n",
    "test = orig_valid.loc[orig_valid['lang'] == \"es\"]\n",
    "\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "18ea10aa-3e43-48b9-afb0-74275003f8c6",
    "_cell_guid": "9b6c4089-5d20-49d4-aef1-18d6102eb61a",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "x_train = fast_encode(train, fast_tokenizer, maxlen=MAX_LEN)\n",
    "x_test = fast_encode(test, fast_tokenizer, maxlen=MAX_LEN)\n",
    "\n",
    "x_test_en = fast_encode_en(test, fast_tokenizer, maxlen=MAX_LEN)\n",
    "\n",
    "y_train = train.toxic.values\n",
    "y_test = test.toxic.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e2664cf1-ff9e-4406-9f68-c1f67d307c86",
    "_cell_guid": "4752cc70-c1ab-45e7-b303-472a3c8b27a7",
    "trusted": true
   },
   "source": [
    "## Build datasets objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "de8a495f-9c90-4b1c-8c9f-59c3866e1476",
    "_cell_guid": "18588435-c06b-4252-a358-fe4d9a31f34d",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((x_train, y_train))\n",
    "    .repeat()\n",
    "    .shuffle(2048)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "test_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((x_test, y_test))\n",
    "    .batch(BATCH_SIZE)\n",
    "    .cache()\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "test_dataset_en = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((x_test_en, y_test))\n",
    "    .batch(BATCH_SIZE)\n",
    "    .cache()\n",
    "    .prefetch(AUTO)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for i in train_dataset:\n",
    "    print (i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "864c3378-36c1-42ed-ac3d-ebf62d383c91",
    "_cell_guid": "33fac876-37f1-4440-a46f-fbaea8b299c6",
    "trusted": true
   },
   "source": [
    "## Load model into the TPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e48e471a-9c20-472f-a2fe-159769a6797c",
    "_cell_guid": "56b0f046-f936-4e47-a883-a87367ebaa64",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "with strategy.scope():\n",
    "    transformer_layer = (\n",
    "        transformers.TFBertModel\n",
    "        .from_pretrained('bert-base-multilingual-cased')\n",
    "    )\n",
    "    model = build_model(transformer_layer, max_len=MAX_LEN)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e0492c2f-375e-4134-9516-f77e50c70873",
    "_cell_guid": "48b4a99f-0e51-4435-b387-59fbe3c17c8d",
    "trusted": true
   },
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a665eb6b-2eb2-45d5-a181-f4df033ab7ac",
    "_cell_guid": "537b1e9f-15d5-4273-b28a-31e64f463451",
    "trusted": true
   },
   "source": [
    "First, we train on the subset of the training set, which is completely in English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dd683a4a-6be4-471e-8875-6d9185d6b8f1",
    "_cell_guid": "1a8a36fe-398f-4f35-932c-48a1746b7f85",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "n_steps = x_train[0].shape[0] // BATCH_SIZE\n",
    "train_history = model.fit(\n",
    "    train_dataset,\n",
    "    steps_per_epoch=n_steps,\n",
    "    epochs=EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "with strategy.scope():\n",
    "    \n",
    "    clf_puller_model = build_clf_puller_model(model)\n",
    "clf_puller_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!wget -O 3000_words.txt --no-check-certificate \"https://drive.google.com/uc?export=download&id=1F6DKWWdi5G95jtiQt79Kz9vHXV7HajFG\"\n",
    "!wget -O en_3000_glosses.txt --no-check-certificate \"https://drive.google.com/uc?export=download&id=1-4Bhj5BUf56KlesmF4WySASp8kFA1o9v\"\n",
    "!wget -O es_3000_glosses.txt --no-check-certificate \"https://drive.google.com/uc?export=download&id=1-IqUpetghkeOfCnFA_gGwwzD2zyl_EYd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "with open (\"en_3000_glosses.txt\", \"r\") as en_f:\n",
    "  with open (\"es_3000_glosses.txt\", \"r\") as es_f:\n",
    "    with open (\"3000_words.txt\", \"r\") as words_f:\n",
    "      en_lines = en_f.read().splitlines()\n",
    "      es_lines = es_f.read().splitlines()\n",
    "      words_lines = words_f.read().splitlines()\n",
    "      en_glosses = []\n",
    "      es_glosses = []\n",
    "      words = []\n",
    "      for i in range (len(words_lines)):\n",
    "        en_gloss = en_lines[i]\n",
    "        es_gloss = es_lines[i]\n",
    "        word = words_lines[i]\n",
    "\n",
    "        if en_gloss != \"-\" and es_gloss != \"-\":\n",
    "          en_glosses.append(en_gloss)\n",
    "          es_glosses.append(es_gloss)\n",
    "          words.append(word)\n",
    "\n",
    "en_df = {\n",
    "    \"comment_text\" : en_glosses,\n",
    "    \"lang\" : [\"en\" for i in en_glosses]\n",
    "    \n",
    "}        \n",
    "\n",
    "en_gloss_df = pd.DataFrame (en_df)\n",
    "\n",
    "\n",
    "es_df = {\n",
    "    \"comment_text\" : es_glosses,\n",
    "    \"lang\" : [\"es\" for i in es_glosses]\n",
    "    \n",
    "}  \n",
    "\n",
    "es_gloss_df = pd.DataFrame (es_df)\n",
    "\n",
    "en_gloss_data = fast_encode(en_gloss_df, fast_tokenizer, maxlen=MAX_LEN)\n",
    "es_gloss_data = fast_encode(es_gloss_df, fast_tokenizer, maxlen=MAX_LEN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a9b23985-4908-4fd6-a56f-b1f08caba0c1",
    "_cell_guid": "2e3031ae-9d43-4411-856b-363e212abbc1",
    "trusted": true
   },
   "source": [
    "Now that we have pretty much saturated the learning potential of the model on english only data, we train it for one more epoch on the `validation` set, which is significantly smaller but contains a mixture of different languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "71e57566-7fd6-4e8e-b7c1-06156fbdace7",
    "_cell_guid": "974a8b15-b63e-4921-9844-24ba808165d1",
    "trusted": true
   },
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6d6c8e88-32cc-48fc-abb3-34727f687e5e",
    "_cell_guid": "71dccda2-ad32-490c-a74b-ae40066ea1a4",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "en_clf = clf_puller_model.predict(en_gloss_data, verbose=1)\n",
    "es_clf = clf_puller_model.predict(es_gloss_data, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "en_centroid = np.average(en_clf, axis=0)\n",
    "es_centroid = np.average(es_clf, axis=0)\n",
    "\n",
    "\n",
    "en_centered = en_clf-en_centroid\n",
    "es_centered = es_clf-es_centroid\n",
    "# scale = np.average((np.sum(en_centered**2, axis=1)**0.5)/(np.sum(es_centered**2, axis=1)**0.5))\n",
    "\n",
    "# scale\n",
    "H = np.dot((en_centered).T , (es_centered))\n",
    "\n",
    "U, sigma, VT = np.linalg.svd(H)\n",
    "R = np.dot(VT.T, U.T)    \n",
    "# # RS = scale*R\n",
    "orig_diff = en_centered - es_centered\n",
    "\n",
    "diff = (np.dot(es_centered, R))-en_centered\n",
    "for i in range(diff.shape[0]):\n",
    "  \n",
    "  print (np.linalg.norm(diff[i]) - np.linalg.norm(orig_diff[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "scale_rotate = np.stack ([np.identity(R.shape[0]), R]).astype(np.float32)\n",
    "vec_to_center = np.stack ([en_centroid, es_centroid]).astype(np.float32)\n",
    "vec_to_translate = en_centroid.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "with strategy.scope():\n",
    "    \n",
    "    language_senstive_model = build_language_senstive_model(model, scale_rotate, vec_to_center, vec_to_translate, max_len=512)\n",
    "language_senstive_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(language_senstive_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "n_steps = x_train[0].shape[0] // BATCH_SIZE\n",
    "train_history = language_senstive_model.fit(\n",
    "    train_dataset,\n",
    "    steps_per_epoch=n_steps,\n",
    "    epochs=EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "predictions = language_senstive_model.predict(test_dataset_en, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "predictions = np.squeeze(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, np.round(predictions))\n",
    "\n",
    "# predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
