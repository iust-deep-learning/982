{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csBoPLPpWgbX",
        "colab_type": "text"
      },
      "source": [
        "# Assignment #2 - Getting Started with Keras\n",
        "\n",
        "\n",
        "Deep Learning / Spring 1399, Iran University of Science and Technology\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kuoO2gzW1ZX",
        "colab_type": "text"
      },
      "source": [
        "**Please pay attention to these notes:**\n",
        "<br><br>\n",
        "\n",
        "\n",
        "- **Assignment Due: ** 1399/01/15 23:59:00\n",
        "- If you need any additional information, please review the assignment page on the course website.\n",
        "- The items you need to answer are highlighted in red and the coding parts you need to implement are denoted by:\n",
        "```\n",
        "########################################\n",
        "#     Put your implementation here     #\n",
        "########################################\n",
        "```\n",
        "- We always recommend co-operation and discussion in groups for assignments. However, each student has to finish all the questions by him/herself. If our matching system identifies any sort of copying, you'll be responsible for consequences. So, please mention his/her name if you have a team-mate.\n",
        "- Students who audit this course should submit their assignments like other students to be qualified for attending the rest of the sessions.\n",
        "- Finding any sort of copying will zero down that assignment grade and also will be counted as two negative assignment for your final score.\n",
        "- When you are ready to submit, please follow the instructions at the end of this notebook.\n",
        "- If you have any questions about this assignment, feel free to drop us a line. You may also post your questions on the course Forum page.\n",
        "- You must run this notebook on Google Colab platform, it depends on Google Colab VM for some of the depencecies.\n",
        "- **Before starting to work on the assignment Please fill your name in the next section *AND Remember to RUN the cell.* **\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "\n",
        "Course Forum: [https://groups.google.com/forum/#!forum/dl982/](https://groups.google.com/forum/#!forum/dl982/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-j0gHRBy3tV",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_deocqkDyxox",
        "colab_type": "text"
      },
      "source": [
        "Fill your information here & run the cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHmlUlfVy9ia",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Enter your information & \"RUN the cell!!\" { run: \"auto\" }\n",
        "student_id =   0#@param {type:\"integer\"}\n",
        "student_name = \"\" #@param {type:\"string\"}\n",
        "\n",
        "print(\"your student id:\", student_id)\n",
        "print(\"your name:\", student_name)\n",
        "\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "ASSIGNMENT_PATH = Path('asg02')\n",
        "ASSIGNMENT_PATH.mkdir(parents=True, exist_ok=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BSSDskiksXu",
        "colab_type": "text"
      },
      "source": [
        "## 1. Keras Basics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1qnHUhdky36",
        "colab_type": "text"
      },
      "source": [
        "In this part of the assignment, you'll become familliar with basic principles of coding in Keras! \n",
        "There are 3 main APIs in keras that you can use depending on the task at hand. These 3 APIs are: Sequential API, Functional API and Model subclassing. You'll implement simple models using all these APIs and will learn when to use each.\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4GLZ3S_puz4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwq4sluJxqgH",
        "colab_type": "text"
      },
      "source": [
        "### 1.1 Sequential API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXs6_K0fxvKi",
        "colab_type": "text"
      },
      "source": [
        "Sequential API is the simplest API to use but the downside is that you can only train simple models with it! You can only implement models that are basically a stack of layers which most of the time isn't the case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOGSltxRyRR1",
        "colab_type": "text"
      },
      "source": [
        "In this part, you should implement simple MLPs to train on the California Housing dataset and the digits dataset. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJuesW-LldlN",
        "colab_type": "text"
      },
      "source": [
        "Let's first load the California Housing dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntyK7adjOHsy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "x_CH, y_CH = fetch_california_housing(return_X_y=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siwAo9ybmAhi",
        "colab_type": "text"
      },
      "source": [
        "The Task is to predict the price of houses based on the features presented in the dataset. We've also split the dataset into training, validation and test and done some pre-processing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2Mza7C05YL3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def SplitAndScale(x, y):\n",
        "  x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33)\n",
        "  x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2)\n",
        "  sc = StandardScaler()\n",
        "  x_train = sc.fit_transform(x_train)\n",
        "  x_val = sc.transform(x_val)\n",
        "  x_test = sc.transform(x_test)\n",
        "\n",
        "  return (x_train, x_val, x_test), (y_train, y_val, y_test)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NQvYOdMvVA7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(x_train_CH, x_val_CH, x_test_CH), (y_train_CH, y_val_CH, y_test_CH) = SplitAndScale(x_CH, y_CH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j33uASe2aQeu",
        "colab_type": "text"
      },
      "source": [
        "Note that we don't know at which epoch our model has the best results on the test set, because the test set is supposed to be unseen, but we can use a portion of the training set as validation set and save the model with the best result on validation set! This can be done using Callbacks. Callbacks give you control over the behavior of the model during training and evaluation. You can check out the Keras documentation for more information on Callbacks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_cLgvcJn4hY",
        "colab_type": "text"
      },
      "source": [
        "Now implement a simple MLP for predicting housing prices in California Housing dataset using the Sequential API. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Db9S250fTRPt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.models import load_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aff5lZsi09Fr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_path = 'reg_mlp.h5' \n",
        "\n",
        "checkpoint = keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, mode='min', monitor='val_mean_absolute_error', verbose=0, save_best_only=True)\n",
        "\n",
        "########################################\n",
        "#     Put your implementation here     #\n",
        "########################################\n",
        "\n",
        "reg_mlp.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "reg_mlp.fit(x_train_CH, y_train_CH, batch_size=128, epochs=20, validation_data=(x_val_CH, y_val_CH), callbacks=[checkpoint])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPkHovROztG5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reg_mlp = load_model('reg_mlp.h5')\n",
        "results = reg_mlp.evaluate(x_test_CH, y_test_CH)\n",
        "print('Mean absolute error on test set: ', results[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTdghYR0z4kV",
        "colab_type": "text"
      },
      "source": [
        "For the next part implement an MLP to classify the digits in the digits dataset! This dataset consists of 8$\\times$8 images of digits and each image's label is a number from 0 to 9 which makes it a multiclass classification task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlLQdTH28pZ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import load_digits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9y7xkiAg8wQM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "x_dig, y_dig = load_digits(return_X_y=True)\n",
        "for i in range(6):\n",
        "  plt.subplot(231+i)\n",
        "  plt.imshow(x_dig[i].reshape(8,8), cmap='gray')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOEixHTvUNcO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(x_train_dig, x_val_dig, x_test_dig), (y_train_dig, y_val_dig, y_test_dig) = SplitAndScale(x_dig, y_dig)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAE0BTIx9lyU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_path = 'clf_mlp.h5' \n",
        "\n",
        "checkpoint = keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, mode='max', monitor='val_acc', verbose=0, save_best_only=True)\n",
        "\n",
        "########################################\n",
        "#     Put your implementation here     #\n",
        "########################################\n",
        "\n",
        "clf_mlp.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
        "clf_mlp.fit(x_train_dig, y_train_dig, batch_size=64, epochs=20, validation_data=(x_val_dig, y_val_dig), callbacks=[checkpoint])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTO0J0Ho9zJD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clf_mlp = load_model('clf_mlp.h5')\n",
        "results = clf_mlp.evaluate(x_test_dig, y_test_dig)\n",
        "print('Accuracy on test set: ', results[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QL6NRht_SpMS",
        "colab_type": "text"
      },
      "source": [
        "### 1.2 Functional API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWY0AkkWSvN7",
        "colab_type": "text"
      },
      "source": [
        "Sequential API is a fast way to implement very simple models but to implement more complex models, like models with multiple inputs and outputs or non-sequential models, we have to use the Functional API.<br>\n",
        "Imagine a model like this:<br><br>\n",
        "<center>\n",
        "<img src=\"https://github.com/iust-deep-learning/982/raw/master/static_files/assignments/asg02_assets/WideAndDeep.jpg\"/>\n",
        "\n",
        "</center>\n",
        "<br>\n",
        "\n",
        "You can't create such a model using Sequential API! This model is called a \"wide and deep\" model. The concept is that we want our model to learn simple features as well as deep features. This kind of architecture is used in recommender systems but let's implement it using the Functional API just to see if it will improve the results on the California Housing dataset!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kBvgnQD3N5U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Input, Concatenate\n",
        "from keras import Model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRtqXND66Dxj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_path = 'WideAndDeep.h5' \n",
        "checkpoint = keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, mode='min', monitor='val_mean_absolute_error', verbose=0, save_best_only=True)\n",
        "\n",
        "########################################\n",
        "#     Put your implementation here     #\n",
        "########################################\n",
        "\n",
        "WideAndDeep.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "WideAndDeep.fit(x_train_CH, y_train_CH, batch_size=128, epochs=20, validation_data=(x_val_CH, y_val_CH), callbacks=[checkpoint])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCWtIlk88fm6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "WideAndDeep = load_model('WideAndDeep.h5')\n",
        "results = WideAndDeep.evaluate(x_test_CH, y_test_CH)\n",
        "print('Mean absolute error on test set: ', results[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHROgw0jjs6_",
        "colab_type": "text"
      },
      "source": [
        "### 1.3 Custom Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YC39SVUNkFhn",
        "colab_type": "text"
      },
      "source": [
        "You can find many of the layers you'll need in **keras.layers** but you can always make your own custom layer using keras. In this part you should implement a custom layer called **DenseStack**. This layer accepts a list of layer sizes and activations and creates a stack of Dense layers with those sizes and activations. For example, using **DenseStack([128, 64], ['relu', 'relu'])** is equivalent to using **Dense(128, 'relu')** and then **Dense(64, 'relu')** back to back."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-Qhshq_YjZR",
        "colab_type": "text"
      },
      "source": [
        "Refer to [this](https://keras.io/layers/writing-your-own-keras-layers/) for information on how to create a custom layer. Also, you are free to use any layer in **keras.layers** this part of the assignment( or any other part that involves creating a custom layer or model)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCwnJY-I8wY8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DenseStack(keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, layer_sizes, activations, **kwargs):\n",
        "\n",
        "      super().__init__(**kwargs)\n",
        "      self.layer_sizes = layer_sizes\n",
        "      self.activations = activations\n",
        "\n",
        "\n",
        "    def build(self, input_shape):\n",
        "\n",
        "      ########################################\n",
        "      #     Put your implementation here     #\n",
        "      ########################################\n",
        "\n",
        "      super(DenseStack, self).build(input_shape)\n",
        "\n",
        "    def call(self, X):\n",
        "\n",
        "      ########################################\n",
        "      #     Put your implementation here     #\n",
        "      ########################################\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "\n",
        "      ########################################\n",
        "      #     Put your implementation here     #\n",
        "      ########################################\n",
        "\n",
        "    def get_config(self):\n",
        "   \n",
        "        config = {**super().get_config(),\n",
        "                  'layer_sizes': self.layer_sizes,\n",
        "                  'activations': self.activations}\n",
        "        return config"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkXZoXr73FvZ",
        "colab_type": "text"
      },
      "source": [
        "Now create a new model using the new **DenseStack** layer you just created. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iayEHHetCi58",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_path = 'mlp_DenseStack.h5' \n",
        "\n",
        "checkpoint = keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, mode='max', monitor='val_acc', verbose=0, save_best_only=True)\n",
        "\n",
        "########################################\n",
        "#     Put your implementation here     #\n",
        "########################################\n",
        "\n",
        "mlp_DenseStack.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
        "mlp_DenseStack.fit(x_train_dig, y_train_dig, batch_size=64, epochs=20, validation_data=(x_val_dig, y_val_dig), callbacks=[checkpoint])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OiPQxsVzsz3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mlp_DenseStack = load_model('mlp_DenseStack.h5', custom_objects={'DenseStack': DenseStack})\n",
        "results = mlp_DenseStack.evaluate(x_test_dig, y_test_dig)\n",
        "print('Accuracy on test set: ', results[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLHVirsg3ix1",
        "colab_type": "text"
      },
      "source": [
        "### 1.4 Model Subclassing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yko71SMCkzhs",
        "colab_type": "text"
      },
      "source": [
        "You can make almost any model using the Functional API, but there are cases when you'll need even more flexibility. For example if you have a model that contains loops or conditional branching, you can't use the Functional API because you can only make models that are DAGs(Directed Acyclic Graph) of layers with the Functional API. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uv4mJ3hw5EZC",
        "colab_type": "text"
      },
      "source": [
        "In this section you should create a custom model named **FunnelMLP** that creates an MLP with funnel-like hidden layers, meaning each hidden layer's size is half the size of it's previous layer, for example **512->256->128**. Here are the inputs of the model: <br>\n",
        "- **first_size**: Size of the first hidden layer.\n",
        "- **num_hidden_layers**: Number of hidden layers. So if **first_size=1024** and **num_hidden_layers=4**, hidden layers of the model will be like this: <center>**1024->512->256->128** </center>\n",
        "- **hidden_activation**: A string denoting the activation function used in the hidden layers. Note that all hidden layers in this model have the same activation function.\n",
        "- **num_classes**: Number of classes in a classification task. This should be set to **0** if we are using this model for a regression task. You should choose the size and activation of the last layer based on this parameter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aL2szwFbcftF",
        "colab_type": "text"
      },
      "source": [
        "Refer to [this](https://keras.io/models/about-keras-models/#model-subclassing) for information on how to make custom models using the Model Subclassing API."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5p8rAR_31tP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FunnelMLP(keras.Model):\n",
        "\n",
        "  def __init__(self, first_size, num_hidden_layers, hidden_activation, num_classes, **kwargs):\n",
        "\n",
        "    ########################################\n",
        "    #     Put your implementation here     #\n",
        "    ########################################\n",
        "\n",
        "    \n",
        "  def call(self, inputs):\n",
        "    \n",
        "    ########################################\n",
        "    #     Put your implementation here     #\n",
        "    ########################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeD5AHtfa7cu",
        "colab_type": "text"
      },
      "source": [
        "Train your model on the digits dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0w1DZUtoFJZ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_path = 'FunnelMLP.h5' \n",
        "\n",
        "checkpoint = keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, mode='max', monitor='val_acc', verbose=0, save_best_only=True, save_weights_only=True)\n",
        "\n",
        "########################################\n",
        "#     Put your implementation here     #\n",
        "########################################\n",
        "\n",
        "Funnelmlp.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
        "Funnelmlp.fit(x_train_dig, y_train_dig, batch_size=64, epochs=10, validation_data=(x_val_dig, y_val_dig), callbacks=[checkpoint])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--cesbMlIc0E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Funnelmlp.load_weights('FunnelMLP.h5')\n",
        "results = Funnelmlp.evaluate(x_test_dig, y_test_dig)\n",
        "print('Accuracy on test set: ', results[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gi_Ps4VtbFR-",
        "colab_type": "text"
      },
      "source": [
        "Now train your model on the California Housing dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1qxrg3BFZzPE",
        "colab": {}
      },
      "source": [
        "checkpoint_path = 'regFunnelMLP.h5' \n",
        "\n",
        "checkpoint = keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, mode='min', monitor='val_mean_absolute_error', verbose=0, save_best_only=True, save_weights_only=True)\n",
        "\n",
        "########################################\n",
        "#     Put your implementation here     #\n",
        "########################################\n",
        "\n",
        "regFunnelmlp.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "regFunnelmlp.fit(x_train_CH, y_train_CH, batch_size=128, epochs=20, validation_data=(x_val_CH, y_val_CH), callbacks=[checkpoint])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gmv5F58OZzPR",
        "colab": {}
      },
      "source": [
        "regFunnelmlp.load_weights('regFunnelMLP.h5')\n",
        "results = regFunnelmlp.evaluate(x_test_CH, y_test_CH)\n",
        "print('Mean absolute error on test set: ', results[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxzHAQSEkDPm",
        "colab_type": "text"
      },
      "source": [
        "## 2. Regularization in Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qetW5OYlpkUt",
        "colab_type": "text"
      },
      "source": [
        "We want our model to not only do well on the training set but also on unseen data, in other words we want to have better generalization, even at the cost of increased error on training data. Regularization is the process of reducing a model's error on unseen data and therefore reducing model's variance. These are many regularizaion methods in deep learning and in this part of the assignment, we use three of them in practice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PKdnkvoBkjuW"
      },
      "source": [
        "We'll be using the fashion mnist dataset in this section. This dataset consist of images of 10 categories of clothing and the number of training and test instances is just like the regular mnist!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hH3vJu5ckjue",
        "colab": {}
      },
      "source": [
        "from keras.datasets import fashion_mnist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BsBpSdpukjum",
        "colab": {}
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fJyQdxFgkjur",
        "colab": {}
      },
      "source": [
        "x_train = x_train/255.\n",
        "x_test = x_test/255."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKU2Yyof38Yb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33xtgz5HC9w4",
        "colab_type": "text"
      },
      "source": [
        "Let's train a simple MLP model on fashion mnist dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzsaerKD9Puz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import  Flatten"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66awZDJDC5qs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_path = 'overfitted_mlp.h5' \n",
        "\n",
        "checkpoint = keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, mode='max', monitor='val_acc', verbose=0, save_best_only=True)\n",
        "\n",
        "overfitted_mlp = Sequential()\n",
        "overfitted_mlp.add(Flatten(input_shape=(28, 28)))\n",
        "overfitted_mlp.add(Dense(256, activation='relu'))\n",
        "overfitted_mlp.add(Dense(128, activation='relu'))\n",
        "overfitted_mlp.add(Dense(10, activation='softmax'))\n",
        "\n",
        "overfitted_mlp.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
        "overfit_history = overfitted_mlp.fit(x_train, y_train, batch_size=512, epochs=50, validation_data=(x_val, y_val), callbacks=[checkpoint])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMxWbQomcYfB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "overfitted_mlp = load_model('overfitted_mlp.h5')\n",
        "results = overfitted_mlp.evaluate(x_test, y_test)\n",
        "print('Accuracy on test set: ', results[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrrkg-PQjZq6",
        "colab_type": "text"
      },
      "source": [
        "Let's plot training and validation accuracy and loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cs4qcAZjW79",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def visualize_loss_and_acc(history):\n",
        "  history_dict = history.history\n",
        "  loss_values = history_dict['loss']\n",
        "  val_loss_values = history_dict['val_loss']\n",
        "  acc = history_dict['acc']\n",
        "\n",
        "  epochs = range(1, len(acc) + 1)\n",
        "\n",
        "  f = plt.figure(figsize=(10,3))\n",
        "\n",
        "  plt.subplot(1,2,1)\n",
        "  plt.plot(epochs, loss_values, 'bo', label='Training loss')\n",
        "  plt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n",
        "  plt.title('Training and validation loss')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "\n",
        "\n",
        "  acc_values = history_dict['acc']\n",
        "  val_acc = history_dict['val_acc']\n",
        "\n",
        "  plt.subplot(1,2,2)\n",
        "  plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "  plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "  plt.title('Training and validation accuracy')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKlk2Z-HDm-C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "visualize_loss_and_acc(overfit_history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duYAHYYAjr4U",
        "colab_type": "text"
      },
      "source": [
        "It's clear that this model has overfitted the training set. We'll use three regularization methods in this part to mitigate this problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skSGo6N_pklj",
        "colab_type": "text"
      },
      "source": [
        "### 2.1 Dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUoVoYwh6NH6",
        "colab_type": "text"
      },
      "source": [
        "Dropout is a very popular and computationaly inexpensive regularization method. Dropout basically creates masks over layers and randomly selects some neurons in a layer and turns all their connections off, which is said to help prevent co-adaptaion of neuron. \n",
        "<center>\n",
        "<img src=\"https://github.com/iust-deep-learning/982/raw/master/static_files/assignments/asg02_assets/dropout.PNG\" width=400/>\n",
        "\n",
        "</center>\n",
        "\n",
        "Now use dropout to improve the generalization of the last model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwHOPbOOD2Or",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Dropout"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4j3yB9SCZ85",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_path = 'drop_mlp.h5' \n",
        "\n",
        "checkpoint = keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, mode='max', monitor='val_acc', verbose=0, save_best_only=True)\n",
        "\n",
        "########################################\n",
        "#     Put your implementation here     #\n",
        "########################################\n",
        "\n",
        "drop_mlp.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
        "drop_history = drop_mlp.fit(x_train, y_train, batch_size=512, epochs=50, validation_data=(x_val, y_val), callbacks=[checkpoint])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szQIjPOWCjgF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "drop_mlp = load_model('drop_mlp.h5')\n",
        "results = drop_mlp.evaluate(x_test, y_test)\n",
        "print('Accuracy on test set: ', results[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4Mh9w8-cqya",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "visualize_loss_and_acc(drop_history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRlvlHeURHGg",
        "colab_type": "text"
      },
      "source": [
        "**Question**: Dropout layer is only active during training and is deactivated during testing. So since some connections are off during training, the activation of layers during training is smaller then the activations during test time. How do we make up for this difference?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oybVspWxRfgj",
        "colab_type": "text"
      },
      "source": [
        "<font color=red>Write your answers here</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8L4IZMQ1mn3",
        "colab_type": "text"
      },
      "source": [
        "#### 2.2 Monte Carlo Dropout (***Bonus***)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bWa0JzG21nd",
        "colab_type": "text"
      },
      "source": [
        "The idea behind Monte Carlo dropout is to let dropout be active also during evaluation, but instead of predicting the test set  once, we predict multiple times and average the prediction probabilities to form the final prediction. <br>\n",
        "Now to activate dropout during training, reimplement your model(**drop_mlp** in the last part) using the Functional API and use dropout like this:<br>\n",
        "<center>\n",
        "\n",
        "**Dropout(rate)(x, training=True)**<br>\n",
        "</center>\n",
        "<br>\n",
        "\n",
        "Also use the exact same hyper-parameters as the last model you created (**drop_mlp**) so we can copy the weights of that model into this new model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbMdBalrDPBy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inp = Input(shape=(28, 28))\n",
        "########################################\n",
        "#     Put your implementation here     #\n",
        "########################################\n",
        "\n",
        "MC_mlp = Model(inputs=[inp], outputs=[out])\n",
        "MC_mlp.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIEDWALyBGUY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "drop_mlp = load_model('drop_mlp.h5')\n",
        "MC_mlp.set_weights(drop_mlp.get_weights())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37Hxvms2DO8F",
        "colab_type": "text"
      },
      "source": [
        "**Question**: Evaluate the test set multiple times (run the cell below multiple times). Why is the accuracy different each time?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWyOrWtYDv0w",
        "colab_type": "text"
      },
      "source": [
        "<font color=red>Write your answers here</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HDbDiCZDL0b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### run multiple times\n",
        "results = MC_mlp.evaluate(x_test, y_test)\n",
        "print('Accuracy on test set: ', results[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pWSUqYBkeiy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "result = drop_mlp.evaluate(x_test, y_test)\n",
        "print('Accuracy on test set using regular dropout: ', result[1])\n",
        "y_preds = []\n",
        "for i in range(1000):\n",
        "  y_pred = MC_mlp.predict(x_test)\n",
        "  y_preds.append(y_pred)\n",
        "y_pred = np.mean(np.array(y_preds), axis=0)\n",
        "print('Accuracy on test set using monte carlo dropout: ', accuracy_score(y_test, np.argmax(y_pred, axis=1)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGT5moltEhhJ",
        "colab_type": "text"
      },
      "source": [
        "**Question**: Did Monte Carlo dropout perform better than the regular dropout? explain why."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zv5IZ301E1Yx",
        "colab_type": "text"
      },
      "source": [
        "<font color=red>Write your answers here</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bM1cMnvEV9Rq",
        "colab_type": "text"
      },
      "source": [
        "### 2.3 Noise Robustness"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOAt3r7_Rl41",
        "colab_type": "text"
      },
      "source": [
        "Another method of regularization is adding noise to inputs or hidden layers or weights or in some cases even the outputs of a network.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmcXpxf4T258",
        "colab_type": "text"
      },
      "source": [
        "**Question**: Explain how adding noise to different parts of a neural network can act as a regularizer?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qVC919IURAs",
        "colab_type": "text"
      },
      "source": [
        "<font color=red>Write your answers here</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Qsk-qU1UWX-",
        "colab_type": "text"
      },
      "source": [
        "In this part, we want to add Gaussian noise to the output of hidden layers of the network. You should implement a custom layer named **NoisyDense**. This layer is basically a regular Keras Dense layer but adds gaussian noise with $\\mu=0$ and a specified $\\sigma$ to the output of the Dense."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ko1SvE1WR2L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras.backend as K"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TJwHHNgM3-w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NoisyDense(keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, layer_size, activation, std, **kwargs):\n",
        "\n",
        "      super().__init__(**kwargs)\n",
        "      self.layer_size = layer_size\n",
        "      self.std = std\n",
        "      self.activation = activation\n",
        "\n",
        "\n",
        "    def build(self, input_shape):\n",
        "\n",
        "      ########################################\n",
        "      #     Put your implementation here     #\n",
        "      ########################################\n",
        "\n",
        "\n",
        "      super(NoisyDense, self).build(input_shape)\n",
        "\n",
        "    def call(self, X):\n",
        "      \n",
        "      ########################################\n",
        "      #     Put your implementation here     #\n",
        "      ########################################\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "\n",
        "      ########################################\n",
        "      #     Put your implementation here     #\n",
        "      ########################################\n",
        "\n",
        "    def get_config(self):\n",
        "   \n",
        "        config = {**super().get_config(),\n",
        "                  'layer_size': self.layer_size,\n",
        "                  'activation': self.activation,\n",
        "                  'std': self.std}\n",
        "        return config"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-Zxa2SqeP3l",
        "colab_type": "text"
      },
      "source": [
        "Try to a good value for $\\sigma$ to prevent overfitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZOM_C6Qe04R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### you should change this value \n",
        "sigma = 0.00001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Obx5b_yfYeLJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_path = 'noisy_mlp.h5' \n",
        "\n",
        "checkpoint = keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, mode='max', monitor='val_acc', verbose=0, save_best_only=True)\n",
        "\n",
        "noisy_mlp = Sequential()\n",
        "noisy_mlp.add(Flatten(input_shape=(28, 28)))\n",
        "noisy_mlp.add(NoisyDense(256, activation='relu', std=sigma))\n",
        "noisy_mlp.add(NoisyDense(128, activation='relu', std=sigma))\n",
        "noisy_mlp.add(Dense(10, activation='softmax'))\n",
        "\n",
        "noisy_mlp.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
        "noisy_history = noisy_mlp.fit(x_train, y_train, batch_size=512, epochs=50, validation_data=(x_val, y_val), callbacks=[checkpoint])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "er2eGMJpZXwN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "noisy_mlp = load_model('noisy_mlp.h5', custom_objects={'NoisyDense': NoisyDense})\n",
        "results = noisy_mlp.evaluate(x_test, y_test)\n",
        "print('Accuracy on test set: ', results[1])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IA11AuPlZKIe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "visualize_loss_and_acc(noisy_history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jb6QUf5SxigI",
        "colab_type": "text"
      },
      "source": [
        "# Submission"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1Sb-xU2xpNM",
        "colab_type": "text"
      },
      "source": [
        "Congratulations! You finished the assignment & you're ready to submit your work. Please follow the instructions:\n",
        "\n",
        "1. Check and review your answers. Make sure all of the cell outputs are what you want. \n",
        "2. Select File > Save.\n",
        "3. Run **Make Submission** cell, It may take several minutes and it may ask you for your credential.\n",
        "4. Run **Download Submission** cell to obtain your submission as a zip file.\n",
        "5. Grab the downloaded file (`dl_asg02__xx__xx.zip`) and upload it via https://forms.gle/mTdqSUVx1emSsmo98"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QBH-aiHyeus",
        "colab_type": "text"
      },
      "source": [
        "## Make Submission (Run the cell)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6h64ZtRpynhH",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "! pip install -U --quiet PyDrive > /dev/null\n",
        "# ! wget -q https://github.com/github/hub/releases/download/v2.10.0/hub-linux-amd64-2.10.0.tgz \n",
        "  \n",
        "import os\n",
        "import time\n",
        "import yaml\n",
        "import json\n",
        "\n",
        "from google.colab import files\n",
        "from IPython.display import Javascript\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "asg_name = 'assignment_2'\n",
        "script_save = '''\n",
        "require([\"base/js/namespace\"],function(Jupyter) {\n",
        "    Jupyter.notebook.save_checkpoint();\n",
        "});\n",
        "'''\n",
        "# repo_name = 'iust-deep-learning-assignments'\n",
        "submission_file_name = 'dl_asg02__%s__%s.zip'%(student_id, student_name.lower().replace(' ',  '_'))\n",
        "\n",
        "sub_info = {\n",
        "    'student_id': student_id,\n",
        "    'student_name': student_name, \n",
        "    'dateime': str(time.time()),\n",
        "    'asg_name': asg_name\n",
        "}\n",
        "json.dump(sub_info, open('info.json', 'w'))\n",
        "\n",
        "Javascript(script_save)\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "file_id = drive.ListFile({'q':\"title='%s.ipynb'\"%asg_name}).GetList()[0]['id']\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile('%s.ipynb'%asg_name) \n",
        "\n",
        "! jupyter nbconvert --to script \"$asg_name\".ipynb > /dev/null\n",
        "! jupyter nbconvert --to html \"$asg_name\".ipynb > /dev/null\n",
        "! zip \"$submission_file_name\" \"$asg_name\".ipynb \"$asg_name\".html \"$asg_name\".txt info.json > /dev/null\n",
        "\n",
        "print(\"##########################################\")\n",
        "print(\"Done! Submisson created, Please download using the bellow cell!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRyW0Jx4yiEi",
        "colab_type": "text"
      },
      "source": [
        "## Download Submission (Run the cell)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svzPnugYztkX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files.download(submission_file_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PistsHgLj_1K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}