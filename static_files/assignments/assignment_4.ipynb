{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment_4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_wEtBmL1Hwk",
        "colab_type": "text"
      },
      "source": [
        "# Assignment #4 - Transformer and BERT\n",
        "\n",
        "\n",
        "Deep Learning / Spring 1399, Iran University of Science and Technology\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-_PRgRX1NDO",
        "colab_type": "text"
      },
      "source": [
        "**Please pay attention to these notes:**\n",
        "<br><br>\n",
        "\n",
        "\n",
        "- **Assignment Due: ** 1399/04/17 23:59:00\n",
        "- If you need any additional information, please review the assignment page on the course website.\n",
        "- The items you need to answer are highlighted in red and the coding parts you need to implement are denoted by:\n",
        "```\n",
        "########################################\n",
        "#     Put your implementation here     #\n",
        "########################################\n",
        "```\n",
        "- We always recommend co-operation and discussion in groups for assignments. However, each student has to finish all the questions by him/herself. If our matching system identifies any sort of copying, you'll be responsible for consequences. So, please mention his/her name if you have a team-mate.\n",
        "- Students who audit this course should submit their assignments like other students to be qualified for attending the rest of the sessions.\n",
        "- Finding any sort of copying will zero down that assignment grade and also will be counted as two negative assignment for your final score.\n",
        "- When you are ready to submit, please follow the instructions at the end of this notebook.\n",
        "- If you have any questions about this assignment, feel free to drop us a line. You may also post your questions on the course Forum page.\n",
        "- You must run this notebook on Google Colab platform, it depends on Google Colab VM for some of the depencecies.\n",
        "- **Before starting to work on the assignment Please fill your name in the next section *AND Remember to RUN the cell.* **\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "\n",
        "Course Forum: [https://groups.google.com/forum/#!forum/dl982/](https://groups.google.com/forum/#!forum/dl982/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTZtiOeg1vHP",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lL9OYI1C1wRq",
        "colab_type": "text"
      },
      "source": [
        "Fill your information here & run the cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ra5wTxj62CWc",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Enter your information & \"RUN the cell!!\" { run: \"auto\" }\n",
        "student_id =   0#@param {type:\"integer\"}\n",
        "student_name = \"\" #@param {type:\"string\"}\n",
        "\n",
        "print(\"your student id:\", student_id)\n",
        "print(\"your name:\", student_name)\n",
        "\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "ASSIGNMENT_PATH = Path('asg04')\n",
        "ASSIGNMENT_PATH.mkdir(parents=True, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QilVSubqFXn4",
        "colab_type": "text"
      },
      "source": [
        "## Transformer and BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBrRyCdUFhlj",
        "colab_type": "text"
      },
      "source": [
        "In this assignment, you will:\n",
        "- Implement a simplified BERT from scratch\n",
        "- Visualize attention in your implementd model\n",
        "- Fine-tune a pre-trained BERT model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVQ9Q4lw4_XA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "from tensorflow.keras.initializers import TruncatedNormal\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7xU1WDCtLvd",
        "colab_type": "text"
      },
      "source": [
        "### Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyRw05ButO-4",
        "colab_type": "text"
      },
      "source": [
        "In order to implement BERT, we should first implement the encoder layer of the transformer. An encoder has 2 main sub-layers: multi-headed attention layer and a simple feed-forward layer. The multi-headed attention layer is already implemented (slightly modified version of the one in the Keras tutorial), but you should implement the feedforward sub-layer and the residual connections (Add & Normalize blocks in the picture below) yourself. <br>\n",
        "<br>\n",
        "<center>\n",
        "\n",
        "![](https://github.com/iust-deep-learning/982/raw/master/static_files/assignments/asg04_assets/encoder.PNG)\n",
        "\n",
        "</center>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prl-PotP4gNl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiHeadAttention(layers.Layer):\n",
        "    def __init__(self, hidden_size, num_heads):\n",
        "\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_heads = num_heads\n",
        "        self.projection_dim = hidden_size // num_heads\n",
        "        self.Q = layers.Dense(hidden_size)\n",
        "        self.K = layers.Dense(hidden_size)\n",
        "        self.V = layers.Dense(hidden_size)\n",
        "        self.out = layers.Dense(hidden_size)\n",
        "\n",
        "    def attention(self, query, key, value, mask):\n",
        "        \n",
        "        score = tf.matmul(query, key, transpose_b=True)\n",
        "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "        scaled_score = score / tf.math.sqrt(dim_key)\n",
        "        maxlen = tf.cast(tf.shape(scaled_score)[-1], tf.int64)\n",
        "        m = tf.repeat(mask, maxlen, axis=2) * (-1e9)\n",
        "        scaled_score += m\n",
        "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
        "        output = tf.matmul(weights, value)\n",
        "        return output, weights\n",
        "\n",
        "    def separate_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, inputs, att_mask):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        query = self.separate_heads(self.Q(inputs)  , batch_size)  \n",
        "        key = self.separate_heads(self.K(inputs), batch_size)  \n",
        "        value = self.separate_heads(self.V(inputs) , batch_size) \n",
        "        attention, self.att_weights = self.attention(query, key, value, att_mask)\n",
        "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
        "        concat_attention = tf.reshape(attention, (batch_size, -1, self.hidden_size))\n",
        "        output = self.out(concat_attention)  \n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "558gGbanoBsu",
        "colab_type": "text"
      },
      "source": [
        "**Question**: Why does the transformer use multi-headed attention instead of just a single self-attention?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4DkUpBToQ8v",
        "colab_type": "text"
      },
      "source": [
        "<font color=red> Write your answer here</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCVLJBv951nW",
        "colab_type": "text"
      },
      "source": [
        "#### Feed-Forward Sub-Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRnFXb0WwwSB",
        "colab_type": "text"
      },
      "source": [
        "The feed-forward sub-layer of the encoder has two dense layers. The first dense layer is called the \"intermediate\" layer and the second one is the \"output\" layer whose functionality is to down-project back to the hidden layer size. Dropout is also applied to the output of the intermediate layer. Unlike the original transformer, BERT uses \"GELU\" activation function in the intermediate dense layer. Since there is no GELU activation function in TensorFlow (there is one in TensorFlow Addons but it will crash your session!), you should implement it yourself!\n",
        "\n",
        "Here is the GELU paper: https://arxiv.org/abs/1606.08415 . Or you can just search the internet!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzD7BjELQ--j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "\n",
        "def GELU(x):\n",
        "\n",
        "  ########################################\n",
        "  #     Put your implementation here     #\n",
        "  ########################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gqd6wedZXxzD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FFN(layers.Layer):\n",
        "\n",
        "  def __init__(self, intermediate_size, hidden_size, drop_rate):\n",
        "\n",
        "    super(FFN, self).__init__()\n",
        "    self.intermediate = layers.Dense(intermediate_size, activation=GELU, kernel_initializer=TruncatedNormal(stddev=0.02))\n",
        "    self.out = layers.Dense(hidden_size, kernel_initializer=TruncatedNormal(stddev=0.02))\n",
        "    self.drop = layers.Dropout(drop_rate)\n",
        "\n",
        "  def call(self, inputs):\n",
        "\n",
        "    ########################################\n",
        "    #     Put your implementation here     #\n",
        "    ########################################\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDlwb3Ea6Aqc",
        "colab_type": "text"
      },
      "source": [
        "#### Residual Connections"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4-UMLpDUkFa",
        "colab_type": "text"
      },
      "source": [
        "In the encoder, dropout is applied to each sub-layer's output, then it gets added to the sub-layer's input (residual connection) and finaly goes through a layer normalizaion step. You should implement all the aforementioned steps in the **AddNorm** custom layer in the cell below!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TtnesNMOHUF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AddNorm(layers.Layer):\n",
        "\n",
        "  def __init__(self, LNepsilon, drop_rate):\n",
        "    \n",
        "    super(AddNorm, self).__init__()\n",
        "    self.LN = layers.LayerNormalization(epsilon=LNepsilon)\n",
        "    self.dropout = layers.Dropout(drop_rate)\n",
        "\n",
        "  def call(self, sub_layer_in, sub_layer_out):\n",
        "    ########################################\n",
        "    #     Put your implementation here     #\n",
        "    ########################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKqyg0J_WuTv",
        "colab_type": "text"
      },
      "source": [
        "Now we have everything we need to implement an encoder layer!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16zvGFBo_uaQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(layers.Layer):\n",
        "\n",
        "    def __init__(self, hidden_size, num_heads, intermediate_size, drop_rate=0.1, LNepsilon=1e-12):\n",
        "      \n",
        "      super(Encoder, self).__init__()\n",
        "      self.attention = MultiHeadAttention(hidden_size, num_heads)\n",
        "      self.ffn = FFN(intermediate_size, hidden_size, drop_rate)\n",
        "      self.addnorm1 = AddNorm(LNepsilon, drop_rate)\n",
        "      self.addnorm2 = AddNorm(LNepsilon, drop_rate)\n",
        "        \n",
        "\n",
        "    def call(self, inputs, mask):\n",
        "\n",
        "      ########################################\n",
        "      #     Put your implementation here     #\n",
        "      ########################################\n",
        "\n",
        "    def compute_mask(self, x, mask):\n",
        "\n",
        "      return mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umQ878ho-6Hp",
        "colab_type": "text"
      },
      "source": [
        "### BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLPlDTeRXefl",
        "colab_type": "text"
      },
      "source": [
        "In the previous part, you implemented the encoder layer. We only need two more layers to implement BERT. First layer is the embedding layer. The final embedding for each token in BERT is the addition of three types of embeddings. Aside from token embeddings, there is also segment embeddings and position embeddings. For this assignment we are ignoring the segment embeddings since we only want to do single sentence classification! <br>\n",
        "Unlike the transformer, which uses fixed positional embeddings, BERT uses learned positional embeddings.\n",
        "\n",
        "<br>\n",
        "<center>\n",
        "\n",
        "![](https://github.com/iust-deep-learning/982/raw/master/static_files/assignments/asg04_assets/bert_emb.PNG)\n",
        "\n",
        "</center>\n",
        "<br>\n",
        "\n",
        "Note that layer normalization followed by dropout is applied to the final embeddings (after adding all the embeddings).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCX-g4qjojYr",
        "colab_type": "text"
      },
      "source": [
        "**Question**: What is segment embedding's functionality in BERT?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4I0E6_AOozvd",
        "colab_type": "text"
      },
      "source": [
        "<font color=red> Write your answer here</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTW-F4t9_x24",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BertEmbedding(layers.Layer):\n",
        "\n",
        "    def __init__(self, vocab_size, maxlen, hidden_size):\n",
        "\n",
        "      super(BertEmbedding, self).__init__()\n",
        "      self.TokEmb = layers.Embedding(input_dim=vocab_size, output_dim=hidden_size, mask_zero=True)\n",
        "      self.PosEmb = tf.Variable(tf.random.truncated_normal(shape=(maxlen, hidden_size), stddev=0.02))\n",
        "      self.LN = layers.LayerNormalization(epsilon=1e-12)\n",
        "      self.dropout = layers.Dropout(0.1)\n",
        "\n",
        "    def call(self, inputs):\n",
        "\n",
        "      ########################################\n",
        "      #     Put your implementation here     #\n",
        "      ########################################\n",
        "\n",
        "    def compute_mask(self, x, mask=None):\n",
        "      m = 1-tf.cast(self.TokEmb.compute_mask(x), tf.float32)\n",
        "      m = m[:, tf.newaxis, tf.newaxis, :]\n",
        "      return m"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPjWqcH-ytQP",
        "colab_type": "text"
      },
      "source": [
        "The last layer you need to implement is the \"pooler\". The pooler converts the hidden states of the last encoder layer (which is of shape **[batch_size, sequence_lenght, hidden_size]**) to a vector representation (which is of shape **[batch_size, hidden_size]**) for each input sentence. The pooler does this by simply taking the hidden state corresponding to the first token (a special token in the beggining of each sentence) and feeding it to a dense layer (tanh is used as the activation function of this dense layer in the original implementation). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O719umhMz_UH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Pooler(layers.Layer):\n",
        "\n",
        "    def __init__(self, hidden_size):\n",
        "\n",
        "      super(Pooler, self).__init__()\n",
        "      self.dense = layers.Dense(hidden_size, activation='tanh')\n",
        "\n",
        "    def call(self, encoder_out):\n",
        "\n",
        "      ########################################\n",
        "      #     Put your implementation here     #\n",
        "      ########################################\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FsjGcFxo8JU",
        "colab_type": "text"
      },
      "source": [
        "**Question**: As it was explained earlier, the pooler's job is to create a single vector representation of a sentence (or sentence pair) by taking the hidden state corresponding to the first token. Can you suggest another form of pooling that could work for BERT?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvR3jJIEpkka",
        "colab_type": "text"
      },
      "source": [
        "<font color=red> Write your answer here</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-P8zt_tFojZY",
        "colab_type": "text"
      },
      "source": [
        "Now you should complete the **create_BERT** function in the cell below. This function gets BERT's hyper-parameters as its inputs and return a BERT model. Use the functional api to create the model.<br>\n",
        "Note that the returned model must have two outputs (just like the pre-trained BERTs): \n",
        "- The hidden states of the last encoder layer\n",
        "- Output of the pooler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9tD7UtNfZ4p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_BERT(vocab_size, maxlen, hidden_size, num_layers, num_att_heads, intermediate_size, drop_rate=0.1):\n",
        "\n",
        "  \"\"\"\n",
        "  creates a BERT model based on the arguments provided\n",
        "\n",
        "        Arguments:\n",
        "        vocab_size: number of words in the vocabulary\n",
        "        maxlen: maximum length of each sentence\n",
        "        hidden_size: dimension of the hidden state of each encoder layer\n",
        "        num_layers: number of encoder layers\n",
        "        num_att_heads: number of attention heads in the multi-headed attention layer\n",
        "        intermediate_size: dimension of the intermediate layer in the feed-forward sublayer of the encoders\n",
        "        drop_rate: dropout rate of all the dropout layers used in the model\n",
        "        returns: \n",
        "        model: a BERT model \n",
        "  \"\"\"\n",
        "  \n",
        "  ########################################\n",
        "  #     Put your implementation here     #\n",
        "  ########################################\n",
        "\n",
        "  model = tf.keras.Model(inputs=?, outputs=[?, ?]) \n",
        "\n",
        "  return model\n",
        "\n",
        "  \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKBEBTI6sFKu",
        "colab_type": "text"
      },
      "source": [
        "The Rotten tomatoes critic reviews dataset is used for this assignment. This dataset consists of about 350000 short reviews."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0DJ9nBtXIJi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://github.com/iust-deep-learning/982/raw/master/static_files/assignments/asg04_assets/reviews.zip\n",
        "!unzip reviews.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUn-48AVXbfR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_reviews, test_reviews = pd.read_csv('train_reviews.csv').values[:, 1:], pd.read_csv('test_reviews.csv').values[:, 1:]\n",
        "\n",
        "(train_texts, train_labels), (test_texts, test_labels)  = (train_reviews[:,0],train_reviews[:,1]), (test_reviews[:,0],test_reviews[:,1])  \n",
        "                                                               "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmLKTFEcmA_q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_texts = [s.lower() for s in train_texts]\n",
        "test_texts = [s.lower() for s in test_texts]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMREt2i2siNY",
        "colab_type": "text"
      },
      "source": [
        "We use the subword text tokenizer from TensorFlow datasets to train a tokenizer on the training set. The special token '**[cls]**' is reserved in the vocabulary of the tokenizer so we add it to the beggining of each sentence later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpUCSbRIYG_w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "aprx_vocab_size = 20000\n",
        "cls_token = '[cls]'\n",
        "tokenizer = tfds.features.text.SubwordTextEncoder.build_from_corpus(corpus_generator=train_texts,\n",
        "                                                        target_vocab_size=aprx_vocab_size,\n",
        "                                                        reserved_tokens=[cls_token])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhTvBa9ntO7b",
        "colab_type": "text"
      },
      "source": [
        "Now complete the **encode_sentence** function in the cell below. This function recieves a sentence and an integer denoting the maximum length of the sentence as inputs and returns a list of token ids. Here are the steps to implement this function:\n",
        "- encode the input sentence using the trained tokenizer to receive a token id list\n",
        "- zero-pad the token id list to the maximum length\n",
        "- add the id corresponding to the special token to the beggining of the token id list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzO4yiJSmIRs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode_sentence(s, maxlen):\n",
        "\n",
        "  ########################################\n",
        "  #     Put your implementation here     #\n",
        "  #########################################\n",
        "  return tok_id_list\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cb6KWQfQMffv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_encoding = encode_sentence('This is a test sentence!', 10)\n",
        "assert len(test_encoding) == 10 and test_encoding[:1] == tokenizer.encode(cls_token)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rL-PTRJPYnPb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAXLEN = 32\n",
        "\n",
        "x_train = np.array([encode_sentence(x, MAXLEN) for x in train_texts], dtype=np.int64)\n",
        "x_test = np.array([encode_sentence(x, MAXLEN) for x in test_texts], dtype=np.int64)\n",
        "y_train = train_labels.astype(np.int64)\n",
        "y_test = test_labels.astype(np.int64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBirx1Fbvv-k",
        "colab_type": "text"
      },
      "source": [
        "Now use the functional api and the **create_BERT** function you implemented earlier to create a classifier for the movie reviews dataset.\n",
        "Note that the intermediate layer in the feed-forward sub-layer of the encoders is set to $4\\times H$ in the original BERT implementation, where $H$ is the hidden layer size. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZOW4L9gBqvc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## We use the BERT-Base hyper-parameters\n",
        "hidden_size = 768\n",
        "num_heads = 12\n",
        "num_layers = 12\n",
        "vocab_size = tokenizer.vocab_size  \n",
        "\n",
        "########################################\n",
        "#     Put your implementation here     #\n",
        "########################################\n",
        "model = keras.Model(inputs=?, outputs=?)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwBQt1bFBwYh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(tf.keras.optimizers.Adam(learning_rate=5e-5), \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IwB37mHByJ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    batch_size=128,\n",
        "    epochs=2,\n",
        "    validation_data=(x_test, y_test)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPfMQS1ZsHHk",
        "colab_type": "text"
      },
      "source": [
        "### Attention Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0L3OgxBx5u_5",
        "colab_type": "text"
      },
      "source": [
        "In this section, we'll use [bertviz](https://github.com/jessevig/bertviz) to visualize attention in the BERT model trained in the last section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwGoqnHXadiF",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Run this!\n",
        "import sys\n",
        "\n",
        "!test -d bertviz_repo && echo \"FYI: bertviz_repo directory already exists, to pull latest version uncomment this line: !rm -r bertviz_repo\"\n",
        "# !rm -r bertviz_repo # Uncomment if you need a clean pull from repo\n",
        "!test -d bertviz_repo || git clone https://github.com/jessevig/bertviz bertviz_repo\n",
        "if not 'bertviz_repo' in sys.path:\n",
        "  sys.path += ['bertviz_repo']\n",
        "\n",
        "from bertviz import head_view\n",
        "\n",
        "def call_html():\n",
        "  import IPython\n",
        "  display(IPython.core.display.HTML('''\n",
        "        <script src=\"/static/components/requirejs/require.js\"></script>\n",
        "        <script>\n",
        "          requirejs.config({\n",
        "            paths: {\n",
        "              base: '/static/base',\n",
        "              \"d3\": \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.8/d3.min\",\n",
        "              jquery: '//ajax.googleapis.com/ajax/libs/jquery/2.0.0/jquery.min',\n",
        "            },\n",
        "          });\n",
        "        </script>\n",
        "        '''))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9p5EleW-6EaN",
        "colab_type": "text"
      },
      "source": [
        "In order to use bertviz, we need to get the attention weights in the encoders of the BERT model implemented in the last section. Now you should complete the **get_att_weights** function in the cell below. This function get two inputs, a model (the trained BERT-based model from last section) and a list of tokens (an encoded sentence). Here's what you should do:\n",
        "- Feed the input token list to the model (so attention weights for that input are created)\n",
        "- Access the **att_weights** attribute of the **MultiHeadAttention** sub-layer of each encoder in the model and put them all in a list\n",
        "- Return the list (a list of Tensors)! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUsR5r4Z-Pd7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_att_weights(model, tok_id_list):\n",
        "  \n",
        "########################################\n",
        "#     Put your implementation here     #\n",
        "########################################\n",
        "  return att_weights\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppSgI5b7pw3n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_sent = \"Hello BERT!\"\n",
        "tok_id_list = encode_sentence(test_sent, MAXLEN)\n",
        "att_weights = get_att_weights(model, tok_id_list)\n",
        "assert len(att_weights) == num_layers\n",
        "assert att_weights[0].shape == [1, num_heads, MAXLEN, MAXLEN]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaV7YOtuBQeC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "def get_att_tok(model, sent):\n",
        "\n",
        "  maxlen = model.layers[0].input_shape[0][-1]\n",
        "  encoded_toks = encode_sentence(sent, maxlen)\n",
        "  att_weights = get_att_weights(model, encoded_toks)\n",
        "  pad_start_idx = np.min(np.where(np.array(encoded_toks) == 0))\n",
        "  toks = encoded_toks[:pad_start_idx]\n",
        "  atts = []\n",
        "  for att in att_weights:\n",
        "    layer_att = torch.FloatTensor(att[:, :, :pad_start_idx, :pad_start_idx].numpy())\n",
        "    atts.append(layer_att)\n",
        "  toks = [tokenizer.decode([m]) for m in toks]\n",
        "  return toks, atts\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65xPcS1VIWyc",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Attentoin Heads Visualization\n",
        "sent = \"I hated that movie\"#@param {type:\"string\"}\n",
        "toks, atts = get_att_tok(model, sent.lower())\n",
        "call_html()\n",
        "head_view(atts, toks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "436xoyTY_Ay2",
        "colab_type": "text"
      },
      "source": [
        "### Fine-Tuning Pre-Trained Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gu0rORGdAcGu",
        "colab_type": "text"
      },
      "source": [
        "Up until now, we've only used BERT with randomly intialized weights. But we can achieve far better results by using pre-trained models since they've been pre-trained on huge corpora and already have a resonable understanding of language. <br>\n",
        "In this section, we'll use [Huggingface's Transformers](https://huggingface.co/transformers/) to fine-tune BERT-Base on the movie critic's reviews dataset.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yh-bFeCxuirM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transformers\n",
        "from transformers import BertTokenizer, TFBertModel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEQQvOCyeHC5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Load the tokenizer of BERT-Base\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "## Load the BERT-Base model\n",
        "bert_model = TFBertModel.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hTFdtZUl3mP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode_sentences(s_list, maxlen):\n",
        "  toks, masks = [], []\n",
        "  for x in s_list:\n",
        "    enc = bert_tokenizer.encode_plus(x, max_length=maxlen, pad_to_max_length=True)\n",
        "    toks.append(enc['input_ids'])\n",
        "    masks.append(enc['attention_mask'])\n",
        "  return np.array(toks).astype(np.int64), np.array(masks).astype(np.int64)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_cTPiS3rqxP",
        "colab_type": "text"
      },
      "source": [
        "**Question**: The pre-trained BERT model requires **attention masks** as well as the input token ids. What are these attention masks used for? How did we create them in our own implementation in the first part of the assignment?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVeN-cUFsO_G",
        "colab_type": "text"
      },
      "source": [
        "<font color=red> Write your answer here</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4Aue9h5v-1R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAXLEN = 32\n",
        "\n",
        "train_toks, train_masks = encode_sentences(train_texts, MAXLEN)\n",
        "test_toks, test_masks = encode_sentences(test_texts, MAXLEN)\n",
        "\n",
        "y_train = train_labels.astype(np.int64)\n",
        "y_test = test_labels.astype(np.int64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Hu7O3p-CTnp",
        "colab_type": "text"
      },
      "source": [
        "Read the documentaion of the Huggingface transformer library and create a classifier using BERT-Base in the cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cszj-464xBMk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "toks = tf.keras.layers.Input(shape=(MAXLEN), dtype=tf.int64)\n",
        "masks = tf.keras.layers.Input(shape=(MAXLEN), dtype=tf.int64)\n",
        "\n",
        "########################################\n",
        "#     Put your implementation here     #\n",
        "#######################################\n",
        "\n",
        "pre_trained_model = tf.keras.Model(inputs=[toks, masks], outputs=[?])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2-mNvZ6zLjH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pre_trained_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])\n",
        "pre_trained_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMV19U3v7NZd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pre_trained_model.fit([train_toks, train_masks],\n",
        "          y_train,\n",
        "          batch_size=128,\n",
        "          epochs=1,\n",
        "          validation_data=([test_toks, test_masks], y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWRra63i2oFS",
        "colab_type": "text"
      },
      "source": [
        "# Submission"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQ6iRsAn2rlE",
        "colab_type": "text"
      },
      "source": [
        "Congratulations! You finished the assignment & you're ready to submit your work. Please follow the instructions:\n",
        "\n",
        "1. Check and review your answers. Make sure all of the cell outputs are what you want. \n",
        "2. Select File > Save.\n",
        "3. Run **Make Submission** cell, It may take several minutes and it may ask you for your credential.\n",
        "4. Run **Download Submission** cell to obtain your submission as a zip file.\n",
        "5. Grab the downloaded file (`dl_asg04__xx__xx.zip`) and upload it via https://forms.gle/sE4kFfDNVaDc7S1v8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFYJJJhh3kpj",
        "colab_type": "text"
      },
      "source": [
        "## Make Submission (Run the cell)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBQc5tBQ2sFJ",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "! pip install -U --quiet PyDrive > /dev/null\n",
        "# ! wget -q https://github.com/github/hub/releases/download/v2.10.0/hub-linux-amd64-2.10.0.tgz \n",
        "  \n",
        "import os\n",
        "import time\n",
        "import yaml\n",
        "import json\n",
        "\n",
        "from google.colab import files\n",
        "from IPython.display import Javascript\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "asg_name = 'assignment_4'\n",
        "script_save = '''\n",
        "require([\"base/js/namespace\"],function(Jupyter) {\n",
        "    Jupyter.notebook.save_checkpoint();\n",
        "});\n",
        "'''\n",
        "# repo_name = 'iust-deep-learning-assignments'\n",
        "submission_file_name = 'dl_asg04__%s__%s.zip'%(student_id, student_name.lower().replace(' ',  '_'))\n",
        "\n",
        "sub_info = {\n",
        "    'student_id': student_id,\n",
        "    'student_name': student_name, \n",
        "    'dateime': str(time.time()),\n",
        "    'asg_name': asg_name\n",
        "}\n",
        "json.dump(sub_info, open('info.json', 'w'))\n",
        "\n",
        "Javascript(script_save)\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "file_id = drive.ListFile({'q':\"title='%s.ipynb'\"%asg_name}).GetList()[0]['id']\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile('%s.ipynb'%asg_name) \n",
        "\n",
        "! jupyter nbconvert --to script \"$asg_name\".ipynb > /dev/null\n",
        "! jupyter nbconvert --to html \"$asg_name\".ipynb > /dev/null\n",
        "! zip \"$submission_file_name\" \"$asg_name\".ipynb \"$asg_name\".html \"$asg_name\".txt info.json > /dev/null\n",
        "\n",
        "print(\"##########################################\")\n",
        "print(\"Done! Submisson created, Please download using the bellow cell!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RclPk2VM30Qa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files.download(submission_file_name)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}